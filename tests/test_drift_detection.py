from unittest.mock import patch

import pytest
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.linalg import Vectors
from pyspark.sql import SparkSession

from src.monitoring.data_drift import (
    calculate_statistics,
    compare_distributions,
    monitor_data_drift,
)
from src.monitoring.model_drift import evaluate_model, monitor_model_drift


@pytest.fixture(scope="module")
def spark():
    """
    Fixture for creating a shared Spark session for tests.
    """
    return (
        SparkSession.builder.appName("TestDriftDetection")
        .master("local[1]")
        .getOrCreate()
    )


# -------------------------
# Tests for Data Drift
# -------------------------


def test_calculate_statistics(spark):
    """
    Test that calculate_statistics computes mean and stddev correctly.
    """
    data = spark.createDataFrame([(1.0,), (2.0,), (3.0,)], ["amount"])
    stats = calculate_statistics(data, ["amount"])
    assert stats["amount"]["mean"] == 2.0
    assert stats["amount"]["stddev"] > 0.0


def test_compare_distributions():
    """
    Test that compare_distributions correctly detects drift.
    """
    baseline_stats = {"amount": {"mean": 2.0, "stddev": 1.0}}
    current_stats = {"amount": {"mean": 4.5, "stddev": 1.0}}
    drift_report = compare_distributions(baseline_stats, current_stats)
    assert drift_report["amount"]["drift_detected"] is True


@patch("src.monitoring.data_drift.pd.DataFrame.to_csv")
def test_monitor_data_drift(mock_to_csv, spark, tmpdir):
    """
    Test monitor_data_drift computes drift and generates a report.
    """
    baseline_data = spark.createDataFrame(
        [(1.0, 2.0), (3.0, 4.0)], ["feature1", "feature2"]
    )
    current_data = spark.createDataFrame(
        [(1.5, 2.5), (3.5, 4.5)], ["feature1", "feature2"]
    )

    baseline_path = tmpdir.join("baseline.parquet")
    current_path = tmpdir.join("current.parquet")
    output_path = tmpdir.join("data_drift_report.csv")

    baseline_data.write.parquet(str(baseline_path))
    current_data.write.parquet(str(current_path))

    monitor_data_drift(str(baseline_path), str(current_path), str(output_path))
    mock_to_csv.assert_called_once()


# -------------------------
# Tests for Model Drift
# -------------------------


def test_evaluate_model(spark):
    """
    Test that evaluate_model computes AUC correctly.
    """
    test_data = spark.createDataFrame(
        [
            (0, Vectors.dense([1.0, 2.0, 3.0])),
            (1, Vectors.dense([2.0, 3.0, 4.0])),
        ],
        ["label", "features"],
    )
    rf = RandomForestClassifier(featuresCol="features", labelCol="label", numTrees=5)
    model = rf.fit(test_data)
    auc = evaluate_model(model, test_data)
    assert 0.0 <= auc <= 1.0


def test_monitor_model_drift_content(spark, tmpdir):
    """
    Test the content of the drift report generated by monitor_model_drift.
    """
    test_data = spark.createDataFrame(
        [
            (0, Vectors.dense([1.0, 2.0, 3.0])),
            (1, Vectors.dense([2.0, 3.0, 4.0])),
        ],
        ["label", "features"],
    )

    test_data_path = tmpdir.join("test_data.parquet")
    test_data.write.parquet(str(test_data_path))

    rf = RandomForestClassifier(featuresCol="features", labelCol="label", numTrees=5)
    from pyspark.ml import Pipeline

    pipeline = Pipeline(stages=[rf])

    baseline_model = pipeline.fit(test_data)
    current_model = pipeline.fit(test_data)

    baseline_path = tmpdir.mkdir("baseline_model")
    current_path = tmpdir.mkdir("current_model")
    baseline_model.write().overwrite().save(str(baseline_path))
    current_model.write().overwrite().save(str(current_path))

    output_path = tmpdir.join("model_drift_report.csv")

    monitor_model_drift(
        str(baseline_path), str(current_path), str(test_data_path), str(output_path)
    )

    with open(output_path, "r") as f:
        content = f.read()
        assert "baseline_auc" in content
        assert "current_auc" in content
        assert "drift_detected" in content
